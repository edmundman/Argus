# -*- coding: utf-8 -*-
"""Llama3.1_(8B)-GRPO_with_Minos_Reward.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16kDc04g50ah0rZ2v9z5klQXDNNJgg7k6 # NOTE: Link might become outdated

To run this, press "*"Runtime*"*" and press "*"Run all*"*" on a **free** Tesla T4 Google Colab instance!
<div class="align-center">
<a href="https://unsloth.ai/"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
<a href="https://discord.gg/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord button.png" width="145"></a>
<a href="https://docs.unsloth.ai/"><img src="https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true" width="125"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> ⭐
</div>

To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).

You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)

### News

**Read our [Gemma 3 blog](https://unsloth.ai/blog/gemma3) for what's new in Unsloth and our [Reasoning blog](https://unsloth.ai/blog/r1-reasoning) on how to train reasoning models.**

Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).

### Installation
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
import os
# Ensure transformers is installed
try:
    import transformers
except ImportError:
    print("Installing transformers...")
    if "COLAB_" not in "".join(os.environ.keys()):
#         !pip install transformers datasets accelerate sentencepiece packaging # Added dependencies
          os.system("pip install transformers datasets accelerate sentencepiece packaging")
    else:
#         !pip install --no-deps transformers datasets accelerate sentencepiece packaging # Added dependencies
          os.system("pip install --no-deps transformers datasets accelerate sentencepiece packaging")

if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth vllm
      os.system("pip install unsloth vllm")
else:
    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]
#     !pip install --no-deps unsloth vllm
      os.system("pip install --no-deps unsloth vllm")

# Commented out IPython magic to ensure Python compatibility.
#@title Colab Extra Install { display-mode: "form" }
# %%capture
import os
if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth vllm
      os.system("pip install unsloth vllm")
else:
#     !pip install --no-deps unsloth vllm
      os.system("pip install --no-deps unsloth vllm")
      # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]
      # Skip restarting message in Colab
      import sys, re, requests; modules = list(sys.modules.keys())
      for x in modules: sys.modules.pop(x) if "PIL" in x or "google" in x else None
#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft "trl==0.15.2" triton cut_cross_entropy unsloth_zoo
      os.system("pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo")
      # Ensure necessary dependencies for transformers are installed
#     !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer torch packaging
      os.system("pip install sentencepiece protobuf datasets huggingface_hub hf_transfer torch packaging")

      # vLLM requirements - vLLM breaks Colab due to reinstalling numpy
      f = requests.get("https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt").content
      with open("vllm_requirements.txt", "wb") as file:
          file.write(re.sub(rb"(transformers|numpy|xformers)[^\n]{1,}\n", b"", f))
#     !pip install -r vllm_requirements.txt
      os.system("pip install -r vllm_requirements.txt")

"""### Unsloth

Load up `Llama 3.1 8B Instruct`, and set parameters
"""

from unsloth import FastLanguageModel
import torch
max_seq_length = 1024 # Can increase for longer reasoning traces
lora_rank = 32 # Larger rank = smarter, but slower

# Load the main generator model (Llama 3.1)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "meta-llama/meta-Llama-3.1-8B-Instruct", # Original generator model
    max_seq_length = max_seq_length,
    load_in_4bit = True, # False for LoRA 16bit
    fast_inference = True, # Enable vLLM fast inference
    max_lora_rank = lora_rank,
    gpu_memory_utilization = 0.6, # Reduce if out of memory
)

# Apply PEFT to the generator model
model = FastLanguageModel.get_peft_model(
    model,
    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ], # Remove QKVO if out of memory
    lora_alpha = lora_rank,
    use_gradient_checkpointing = "unsloth", # Enable long context finetuning
    random_state = 3407,
)

# Load the specified reward model (Minos-v1)
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

reward_model_name = "NousResearch/Minos-v1"
print(f"Loading reward model: {reward_model_name}")

# Try loading in float16 to save memory. Might still be too large for free Colab.
# You might need to use 8-bit/4-bit loading if memory issues persist.
# Define labels explicitly as per the Minos-v1 documentation/usage example
id2label = {0: "Non-refusal", 1: "Refusal"}
label2id = {"Non-refusal": 0, "Refusal": 1}

try:
    reward_model = AutoModelForSequenceClassification.from_pretrained(
        reward_model_name,
        num_labels=2,
        id2label=id2label,
        label2id=label2id,
        torch_dtype=torch.float16, # Use float16 to save memory
        device_map="auto" # Automatically use GPU if available
    )
    reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_name)
    # Ensure model is on GPU if device_map="auto" didn't place it there
    if torch.cuda.is_available() and not next(reward_model.parameters()).is_cuda:
         print("Manually moving reward model to GPU.")
         reward_model.to("cuda")
    print("Reward model loaded successfully.")
except Exception as e:
    print(f"Error loading reward model: {e}")
    print("Please check model name and ensure sufficient GPU/CPU memory.")
    reward_model = None
    reward_tokenizer = None

# Define device for reward model inference (use GPU if available)
reward_device = "cuda" if torch.cuda.is_available() and reward_model else "cpu"
if reward_model:
    reward_model.to(reward_device)
    reward_model.eval() # Set model to evaluation mode
    print(f"Reward model is on device: {reward_device}")


"""### Data Prep
<a name="Data"></a>

We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!
"""

import re
from datasets import load_dataset, Dataset

# Load and prep dataset
SYSTEM_PROMPT = """
Respond in the following format:
<reasoning>
...
</reasoning>
<answer>
...
</answer>
"""

XML_COT_FORMAT = """\
<reasoning>
{reasoning}
</reasoning>
<answer>
{answer}
</answer>
"""

def extract_xml_answer(text: str) -> str:
    # Improved extraction to handle missing tags or variations
    if "<answer>" in text and "</answer>" in text:
        try:
            answer = text.split("<answer>", 1)[1].split("</answer>", 1)[0]
            return answer.strip()
        except IndexError:
             return text # Return original if split fails unexpectedly
    # Fallback or alternative extraction if needed
    print(f"Warning: Could not extract answer from: {text[:100]}...") # Log problematic cases
    return text # Or return a specific error indicator

def extract_hash_answer(text: str) -> str | None:
    if "####" not in text:
        return None
    try:
        return text.split("####", 1)[1].strip()
    except IndexError:
        return None


# uncomment middle messages for 1-shot prompting
def get_gsm8k_questions(split = "train") -> Dataset:
    data = load_dataset('openai/gsm8k', 'main', trust_remote_code=True)[split] # type: ignore
    data = data.map(lambda x: { # type: ignore
        'prompt': [
            {'role': 'system', 'content': SYSTEM_PROMPT},
            {'role': 'user', 'content': x['question']}
        ],
        'answer': extract_hash_answer(x['answer'])
    }) # type: ignore
    # Filter out examples where answer extraction failed
    data = data.filter(lambda x: x['answer'] is not None) # type: ignore
    return data # type: ignore

dataset = get_gsm8k_questions()

# Reward functions (keeping the format-based ones)
def int_reward_func(completions, **kwargs) -> list[float]:
    responses = [completion[0]['content'] for completion in completions]
    extracted_responses = [extract_xml_answer(r) for r in responses]
    # Simple check if the extracted answer consists of digits
    rewards = []
    for r in extracted_responses:
        cleaned_r = ''.join(filter(str.isdigit, r)) # Extract digits
        # Check if the cleaned string is purely digits and matches the original stripped string
        rewards.append(0.5 if cleaned_r and r.strip() == cleaned_r else 0.0)
    return rewards

def strict_format_reward_func(completions, **kwargs) -> list[float]:
    """Reward function that checks if the completion has a specific format."""
    # Using re.DOTALL to make '.' match newlines
    pattern = r"^<reasoning>.*?</reasoning>\s*<answer>.*?</answer>\s*$"
    responses = [completion[0]["content"] for completion in completions]
    matches = [re.match(pattern, r, re.DOTALL) for r in responses]
    return [0.5 if match else 0.0 for match in matches]

def soft_format_reward_func(completions, **kwargs) -> list[float]:
    """Reward function that checks if the completion contains the reasoning and answer tags."""
    reasoning_pattern = r"<reasoning>.*?</reasoning>"
    answer_pattern = r"<answer>.*?</answer>"
    responses = [completion[0]["content"] for completion in completions]
    rewards = []
    for r in responses:
        score = 0.0
        # Using re.DOTALL to make '.' match newlines
        if re.search(reasoning_pattern, r, re.DOTALL):
            score += 0.25
        if re.search(answer_pattern, r, re.DOTALL):
            score += 0.25
        rewards.append(score)
    return rewards

def count_xml(text) -> float:
    count = 0.0
    # Be more flexible with whitespace
    if "<reasoning>" in text: count += 0.125
    if "</reasoning>" in text: count += 0.125
    if "<answer>" in text: count += 0.125
    if "</answer>" in text: count += 0.125
    # Penalties might need refinement based on typical failure modes
    # Example: penalize text after closing answer tag
    parts = text.split("</answer>", 1)
    if len(parts) > 1 and parts[1].strip():
         count -= 0.1 # Penalize extraneous text after answer
    return max(0.0, count) # Ensure reward is not negative

def xmlcount_reward_func(completions, **kwargs) -> list[float]:
    contents = [completion[0]["content"] for completion in completions]
    return [count_xml(c) for c in contents]

# Define the reward function using the loaded Minos model
def minos_reward_func(prompts, completions, answer, **kwargs) -> list[float]:
    """
    Reward function using NousResearch/Minos-v1.
    Uses the probability of the 'Non-refusal' class as reward.
    """
    if not reward_model or not reward_tokenizer:
        print("Reward model not loaded. Returning zero reward.")
        return [0.0] * len(completions)

    rewards = []
    responses = [comp[0]['content'] for comp in completions] # Assuming completions is a list of lists of dicts
    questions = [p[-1]['content'] for p in prompts] # Assuming prompts is a list of lists of dicts, last is user

    # Process in batches for efficiency if possible
    batch_size = 8 # Adjust based on GPU memory
    for i in range(0, len(responses), batch_size):
        batch_questions = questions[i:i+batch_size]
        batch_responses = responses[i:i+batch_size]

        # Format input for Minos-v1 based on the example provided
        # <|user|>\n{question}\n<|assistant|>\n{response}
        formatted_inputs = [f"<|user|>\n{q}\n<|assistant|>\n{r}" for q, r in zip(batch_questions, batch_responses)]

        try:
            inputs = reward_tokenizer(
                formatted_inputs,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512 # Ensure max_length is appropriate for Minos
            ).to(reward_device)

            with torch.no_grad():
                outputs = reward_model(**inputs)
                probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
                # Get the probability of "Non-refusal" (index 0)
                non_refusal_probs = probabilities[:, 0].cpu().tolist()
                rewards.extend(non_refusal_probs)

        except Exception as e:
            print(f"Error during reward model inference: {e}")
            rewards.extend([0.0] * len(batch_responses)) # Assign zero reward on error

    # The probability is already in [0, 1]. You might scale it if desired.
    # Example scaling: return [r * 2.0 for r in rewards] # Scale to [0, 2]
    return rewards

"""<a name="Train"></a>
### Train the model

Now set up GRPO Trainer and all configurations!
"""

max_prompt_length = 256 # Max length for the prompt part
max_completion_length = max_seq_length - max_prompt_length # Max length for the completion part

from trl import GRPOConfig, GRPOTrainer
training_args = GRPOConfig(
    learning_rate = 5e-6,
    adam_beta1 = 0.9,
    adam_beta2 = 0.99,
    weight_decay = 0.1,
    warmup_ratio = 0.1,
    lr_scheduler_type = "cosine",
    optim = "paged_adamw_8bit",
    logging_steps = 10, # Log less frequently
    per_device_train_batch_size = 1, # Will be adjusted by Trainer if needed
    gradient_accumulation_steps = 4, # Increase for smoother training & less memory per step
    num_generations = 4, # Decrease if out of memory (needs to be factor of batch size * grad_accum)
    max_prompt_length = max_prompt_length,
    max_completion_length = max_completion_length,
    # num_train_epochs = 1, # Set to 1 for a full training run
    max_steps = 100, # REDUCED FOR TESTING - Adjust for actual training (e.g., 250 or more)
    save_steps = 50, # Save checkpoints more often during testing
    max_grad_norm = 1.0, # Adjusted grad norm
    report_to = "none", # Can use Weights & Biases: "wandb"
    output_dir = "outputs",
    seed = 3407, # For reproducibility
    # Added parameters to potentially help with memory/performance
    bf16=False, # Use float16 if bf16 not supported or causing issues
    fp16=torch.cuda.is_available(), # Use float16 if GPU is available
    gradient_checkpointing=True, # Helps save memory
    remove_unused_columns=False, # Important for custom datasets
)

"""And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column (specifically the one corresponding to `minos_reward_func`) increase!

You might have to wait many steps for significant changes, especially with complex reward models. Be patient!

| Step | Training Loss | reward    | reward_std | completion_length | kl       | rewards / minos_reward_func | ... |
|------|---------------|-----------|------------|-------------------|----------|---------------------------|-----|
| ...  | ...           | ...       | ...        | ...               | ...      | ...                       | ... |
"""

# Ensure dataset is not empty and has the required columns
print(f"Dataset size: {len(dataset)}")
print(f"Dataset features: {dataset.features}")
if len(dataset) == 0:
     raise ValueError("Dataset is empty. Check data loading and filtering steps.")

# Check if reward model loaded correctly before starting trainer
if reward_model is None or reward_tokenizer is None:
    print("Cannot start training without a reward model. Exiting.")
else:
    trainer = GRPOTrainer(
        model = model,
        # tokenizer = tokenizer, # Pass the generator tokenizer
        reward_funcs = [
            minos_reward_func, # New model-based reward
            xmlcount_reward_func, # Keep format rewards
            soft_format_reward_func,
            strict_format_reward_func,
            int_reward_func,
        ],
        args = training_args,
        train_dataset = dataset,
        tokenizer = tokenizer, # Pass the generator tokenizer
    )
    try:
        print("Starting training...")
        trainer.train()
        print("Training finished.")
    except Exception as e:
        print(f"An error occurred during training: {e}")
        # Consider adding cleanup code here if needed (e.g., release GPU memory)

"""<a name="Inference"></a>
### Inference
Now let's try the model we just trained! First, let's first try the model without any GRPO trained:
"""

# Inference without LoRA (Base model)
# Check if model object exists and has fast_generate (in case of errors)
if 'model' in locals() and hasattr(model, 'fast_generate'):
    text_base = tokenizer.apply_chat_template([
        {"role" : "user", "content" : "Calculate pi."},
    ], tokenize = False, add_generation_prompt = True)

    from vllm import SamplingParams
    sampling_params = SamplingParams(
        temperature = 0.8,
        top_p = 0.95,
        max_tokens = 512, # Reduced max tokens for testing
    )

    # Generate output from the base model (no LoRA)
    try:
        print("Running base model inference...")
        output_base = model.fast_generate(
            [text_base],
            sampling_params = sampling_params,
            lora_request = None, # No LoRA adapter for base model inference
        )[0].outputs[0].text
        print("Base Model Output:\n", output_base)
    except Exception as e:
        print(f"Error during base model inference: {e}")
        print("Skipping base model inference.")
else:
    print("Generator model not loaded correctly. Skipping base inference.")


"""And now with the LoRA we just trained with GRPO - we first save the LoRA first!"""

# Save the trained LoRA adapter (only if training completed successfully)
if 'trainer' in locals() and trainer.state.is_world_process_zero: # Check if trainer exists and is main process
     try:
          print("Saving LoRA adapter...")
          model.save_lora("grpo_saved_lora")
          print("LoRA adapter saved.")
     except Exception as e:
          print(f"Error saving LoRA adapter: {e}")

"""Now we load the LoRA and test:"""

# Inference with the trained GRPO LoRA
if 'model' in locals() and hasattr(model, 'fast_generate'):
    text_grpo = tokenizer.apply_chat_template([
        {"role" : "system", "content" : SYSTEM_PROMPT}, # Include system prompt for formatted output
        {"role" : "user", "content" : "Calculate pi."},
    ], tokenize = False, add_generation_prompt = True)

    # Reuse SamplingParams defined earlier

    # Generate output with the GRPO LoRA adapter
    try:
        # Ensure the LoRA adapter exists before trying to load it
        if os.path.exists("grpo_saved_lora"):
            print("Loading saved LoRA adapter for inference...")
            lora_adapter = model.load_lora("grpo_saved_lora")
            print("Running GRPO model inference...")
            output_grpo = model.fast_generate(
                [text_grpo], # Use the correctly formatted prompt
                sampling_params = sampling_params,
                lora_request = lora_adapter,
            )[0].outputs[0].text
            print("\nGRPO Model Output:\n", outpu